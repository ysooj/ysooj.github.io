---
title: "[GitHub Blog] TIL 35"

writer: ysooj
categories:
- GitHub Blog
tags: [Blog, jekyll, Github, Git, markdown, TIL]

toc: true
toc_sticky: true

date: 2024-11-11
---

## **LLM ( Large Language Model )**

**LLM**은 대규모 언어 모델이다. 이는 번역, 작문, 코딩까지 그 활용 범위가 굉장히 넓기 때문에 서비스의 형태로도 굉장히 다양하게 제공되고 있다. **RAG, Vector DB, LangChain**과 같은 기술들은 LLM을 좀 더 편하게 쓰고, LLM이 근본적으로 가지고 있는 한계점을 극복하기 위해 나온 기술들이다. 대형 언어 모델은 말도 안 되는 양의 크기의 인공지능에, 말도 안 되는 양의 텍스트 데이터를 학습시켜서, 마치 사람같은 자연어 처리 능력을 갖게 만든 모델을 말한다.

### **LLM의 동작 원리**

LLM을 동작시키기 위해서는 세 가지 단계가 필요하다.

첫 번째로, **학습**이다. LLM도 인공지능이기 때문에 학습 단계가 포함돼야 하는 것이다. LLM은 대규모 텍스트 데이터셋을 사용한다는 특징이 있다. 모델 자체의 크기가 굉장히 크고, 사람의 자연어는 적은 양의 데이터로는 그 특성을 파악하기 힘들기 때문에, 대규모 데이터가 필수적이다.

이렇게, 학습이 완료된 인공지능, 즉 LLM은 **추론**의 단계를 거치게 된다. 사용자로부터 특정한 입력을 받으면, 그에 맞는 행동을 수행하게 된다.

마지막 단계로는, 항상 필수적이지는 않은데, 특정한 서비스를 만들거나, 조금 최적화된 LLM을 만든다고 할 때 필요한 작업인 **미세 조정**이다. 예를 들어 콜 센터나 의료나 법률과 같은 특수한 분야에 맞는 상담을 해주기 위해서는, 그 분야에 맞는 데이터를 통해, 좀 더 미세하게 학습하는, 파인 튜닝이라는 작업이 필요하게 된다.

#### **LLM의 랜덤성과 조건성**

LLM은 기본적으로 두 가지 원리로 돌아가게 된다. 바로 **랜덤성**과 **조건성**이다.

LLM은 기본적으로 확률에 기반한다. 모델이 동작할 때마다, 새로운 문장을 생성하게 된다. 또 LLM은, 조건부 확률을 기반으로 결과를 만들어낸다. 이를 통해 어느 정도는 사용자가 원하는 방식대로 동작할 수 있게 도와준다.

### **LLM의 원리 요약**

LLM은 대규모 데이터를 통해 학습을 진행해서, 사람의 말의 문맥을 이해할 수 있다. 랜덤성과 조건성을 바탕으로 결과를 출력하기 때문에, 창의적인 업무도 가능하게 된다. 만일 좀더 특정 목적으로 사용하고 싶다면, Fine-tuning이라고 하는 기능을 사용하면 된다.

추가적으로 알아두면 좋은 것을 알아보자. **학습되지 않은 데이터**에 대해서는 어떻게 되는 지에 대해 살펴보자. LLM은 입력에 따라서 확률적으로 출력을 생성하기 때문에, 입력에 있는 정보도 활용할 수 있다. 이러한 원리로 돌아가는 게 **RAG**고, 이 RAG를 동작시키기 위해 데이터베이스 역할을 해주는 것이 **Vector DB**다.

---

이번에는 LLM 시스템을 구축하기 위한 핵심 기술 요소들을 살펴볼 것이다.

### **LLM ( Large Language Model ) 의 역할 및 중요성**

LLM은 글로써 수행할 수 있는 작업은 모두 수행할 수 있다. 좀 더 상세히 말하면, 학습된 데이터에서 맥락을 파악할 수 있었던 경우, LLM은 그 작업을 수행할 수 있다. 물론 데이터셋의 하나의 데이터 안에 모두 포함돼야 하는 건 아니고, 데이터셋 전반에 걸쳐서 정보가 퍼져 있더라도, 맥락을 이해할 수 있기 때문에, 다양한 작업을 수행할 수 있다. 다르게 말하면, 완전하게 주어지지 않은, 전혀 정보가 없는 특정 함수나 API 등은 사용하지 못한다. 이를 극복하기 위한 다양한 기술 중 대표적인 기법이 바로 RAG다.

### **RAG ( Retrieval-Augmented Generation )**

RAG는 Retrieval-Augmented Generation의 약자로, 검색을 기반으로 답을 생성해내는 기법이다. LLM은 많은 데이터를 학습했지만, 최신 정보나 특정 도메인 지식에 대한 한계를 가질 수 있다. RAG는 LLM이 100% 답을 생성해내는 것이 아니라, 데이터베이스에 내가 원하는 정보들을 저장해놓고, 그 정보를 사용자 입력에 따라서 분석한 다음 유사한 문서를 찾는다. 그리고 나서 그 관련 정보를 LLM에게 전달함으로써, 그 정보를 바탕으로 답변을 생성하는 방식을 말하게 된다.

RAG의 동작 원리는 **질문 입력,** **문서 검색 (Retrieval),** **답변 생성 (Generation)** 라고 할 수 있다.

RAG 의 **장점**을 살펴보자. RAG를 쓰면 최신 정보도 활용할 수 있고, 또한 **특정 도메인**, 개인 정보나, 보험, 혹은 가전제품과 같은 **정보**들을 **제공**해줄 수 있다. 또한 필요한 정보만을 검색해오기 때문에, 다른 시스템에 비해 상당히 빠르다.Vector DB는 동작 속도 자체도 굉장히 빠른 편인데, 거기에 필요한 정보만을 얻어 오기 때문에 더더욱 빠르다고 **효율적**이라는 특징이 있다.

#### **Vector DB ( 벡터 데이터베이스 )**

그러면 RAG 시스템을 구축할 때는, 데이터베이스 구축이 필요하다. 이때 사용되는 데이터 베이스가 Vector DB다. 사용자로부터 입력이 들어오면, 그와 유사한 문장이나 문단, 문맥을 찾아서, 이걸 바탕으로 정답을 생성해내는 기법이 RAG 기법인데, 입력이 들어왔을 때, 그 문장을 찾을 데이터베에스가 필요하다. 일반적으로 RAG에서는 Vector DB라는 것을 사용한다. Vector DB는 숫자의 나열인 벡터를 저장하고, 이걸 빠르게 찾을 수 있는 데이터베이스를 말한다. **임베딩**을 진행하면 텍스트는 숫자의 나열인 벡터로 바뀌기 때문에, Vector DB에 넣을 수 있는 형태가 된다.

Vector DB를 사용하는 데에 여러 **장점**이 있다. 첫 번째로, 문장 단위로 그대로 저장해놨을 때, 유사한 문장을 찾기가 쉽지 않다. 그러나 임베딩은, 단어나 문장, 즉 텍스트를, 유사한 텍스트는 유사한 숫자 나열을 갖도록 바꾸는 기법을 말하기 때문에, 자연스럽게 숫자 간의 유사도만으로도, 유사한 문장을 찾을 수 있다는 정점이 생기게 된다.

또한 굉장히 빠르다. 이전 내용에서 RAG가 굉장히 빠르다고 했는데, 그 장점이 Vector DB에서 기인한다고 할 수 있다.

#### **Vector DB의 동작 과정**

Vector DB의 동작 과정도 살펴보자.

먼저 **임베딩을 생성**해야 한다. 처음에 입력할 데이터가 필요하다. 이러한 텍스트 정보를 벡터로 변환한다. 벡터로 변환하는 방법도 굉장히 다양한데, 일반적으로 **임베딩 기법**이라고 부른다.

이렇게 변환된 **벡터**는 Vector DB라는 곳에 **저장**되게 된다. 이 벡터들은 의미상 유사하다면 유사한 숫자 나열을 갖게 된다.

그렇기 때문에 **검색**도 굉장히 빠르게 되는 것이다. 그리고 **결과를 제공**한다.

Vector DB의 **장점**을 살펴보자. 문장을 검색할 때, 단순히 키워드를 매칭하는 게 아니라, **의미 단위로 검색**할 수 있다. 임베딩 덕분이다. 또, CPU를 활용하는 경우도 있지만, GPU 활용까지도 가능하기 때문에, **성능적으로 굉장히 훌륭**하다.

#### **LangChain**

그런데 이러한 기법들을 LLM에 결합시키기란 쉽지 않다. 이를 편하게 사용하는 방법 중 대표적인 기능이   **LangChain**이다. 다양한 LLM과 다양한 기법들을 결합할 수 있게 도와주어서, 강력한 언어 기반 서비스를 쉽게 만들 수 있게 도와준다. LLM의 기능을 확장해주고, 다양한 데이터베이스 혹은 API와 결합할 수 있도록 도와준다. 대표적인 기능은 RAG 기법, 작업 흐름의 자동화가 있다.

LangChain의 동작 원리는 간단하게, LLM을 동작시킬 때 여러 가지 순차적인 단계나, 꼬여있는 단계가 있다면, 이걸 굉장히 쉽게 할 수 있게 해준다고 이해하면 된다.

---

이번에는 OpenAI Playground라는 CahtGPT의 실험 환경에 대해 배울 것이다

### **OpenAI Playground**

OpenAI Playground는 간단히 말하면, 프롬프트의 실험 환경이다. 모델도 설정하고 다양한 파라미터도 설정할 수 있다. OpenAI Playground는 굉장히 직관적으로 돼있기 때문에 우리가 모델 선택이나 하이퍼 파라미터 튜닝, 혹은 프롬프팅을 실험할 때, 본격적으로 코드를 작성하지 않고도 굉장히 유용하게 실험할 수 있게 된다. OpenAI Playground의 주요 목적은 실제 구현이 아니다. 오히려 여러 가지 실험을 편하게 하는 데 맞춰져 있다.

#### **OpenAI Playground의 주요 설정**

여러 설정값들에 대해 살펴보자.

첫 번째로, **모델을 선택**하는 부분이다. GPT의 현재 버전부터 레거시 버전까지, 굉장히 다양한 버전을 선택할 수 있도록 제공해주고 있다. 이를 통해 본인의 환경에 맞는 적당한 모델을 찾을 수 있다.

**온도**라는 파라미터가 있다. 이는 램덤성을 조절하는 매개변수다. 값이 높을 수록 창의적이고 예측 불가능한 응답이 나온다. 그리고 값이 낮을 수록 일관적이고 안정적인 응답이 나온다. 그렇다보니, 창의적인 행동을 할 때는 Temperature 값을 높게,  안정적인 답변을 내놓을 때는 낮은 Temperature 값을 사용하는 게 좋다. 일반적으로 인공지능의 파라미터는 극단치의 값을 선택하는 것보다, 극단치에서 약간 떨어진 값을 최대치로 삼는 게 좋다. 그 값은 당연히 상황에 따라 다르다.

**토큰**은 GPT에서 사용하는 언어 단위다. 이 값을 조절함으로써 텍스트의 **길이**를 조절할 수 있다. 짧은 응답이 필요한 경우 작은 값을, 긴 응답이 필요한 경우 높은 값을 설정하면 된다.

**탑-피**는 응답의 다양성을 제어하는 또 다른 파라미터다. 뭐든 가능한 답변을 고려할지, 아니면 확률적으로 몇 프로에 해당하는 답변들만 선택할지, 이런 식으로 응답의 선택지를 줄인다고 생각하면 된다.

**프롬프트 종료**를 위해서 특정 단어나 기호를 설정할 수 있다. 응답이 해당 기호에 도달하면 응답이 멈추게 된다.

예) 답변이 끝날 때마다 "END"라는 단어가 나오게 할 수 있습니다.

#### **User, Assistant, System 역할 설정**

**User(사용자)**는 말 그대로 대화의 주체다. 프롬프트를 입력하는 역할을 담당하고, 주로 명령을 내린다.

**Assistant(도우미)**는 ChatGPT다. 유저의 요청에 응답하는 역할을 맡는데, 여기서는 GPT 모델이 답변을 하기 때문에 GPT를 의미하게 된다.

**System(시스템)**은 대화에 전반적인 규칙이나 지침을 제공하는 역할을 한다. 이 시스템 명령어는 전체적으로 대화에 관여하게 된다. 그래서 하나의 입력이 아니라, 전체 대화에 영향을 줘야하는 경우, 시스템 명령어에 프롬프트를 적어준다고 생각하면 된다.

#### **주의 사항**

Playground는 토큰마다 **비용**이 발생한다. LLM에는 할루시네이션 현상이 좀 있어서, 잘못하면, LLM이 잘못된 정보나 혹은 정보를 적당히 조합해서 거짓 정보를 생성해낼 수 있다. 그래서 우리가 사용할 때 주의가 필요하다. 또 개인정보나 민감한 데이터는  Playground에 입력하지 않는 게 좋다. 응답이 너무 긴 경우, 응답이 잘릴 수도 있다. 그래서 우리가 실제로 구현을 할 때는, 길이를 제한한다고 말을 하든지, 너무 긴 경우 여러 번에 나눠서 응답을 생성하게 유도해야 한다.