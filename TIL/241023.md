#### **과적합(OverFitting) 방지 기법**

딥러닝의 큰 문제 중 하나이다. 우리는  모델이 얼마나 학습을 해야 하는 지를 알기가 어렵다. 이를 판별할 방법이 없으니까, 과적합 방지 기법이 나온 것이다. 대표적인 방법으로 정규화와 드롭아웃, 조기 종료와 데이터 증강이 있다.

**정규화(Normalization)**는 모델의 학습을 안정화하고 성능을 향상시키기 위해 데이터를 일정한 범위로 조정하는 기법이다. 여기서 데이터는 입력 데이터뿐만 아니라, 여러 개, 즉 딥러닝에서 사용되는 다양한 데이터 형태를 모두 포함한다. 그렇기 때문에 정규화의 종류는 꽤 많다. **배치 정규화(Batch Normalization)**는 미니 배치에서 활성화 값을 정규화하는 기법이다.  **레이어 정규화(Layer Normalization)**는 각 레이어의 활성화 값을 정규화하는데 주로 RNN이 많이 사용된다.

**드롭아웃(Dropout)**은 학습 과정에서 무작위로 특정 뉴런을 비활성화한다. 드롭아웃은 학습 시에만 적용되며, 평가 시에는 모든 뉴런을 활성화한다.

**조기 종료(Early Stopping) 기법**은 데이터를 학습 데이터, 검증 데이터, 평가 데이터 이 3개로 나눈다.

**데이터 증강(Data Augmentation) 기법**은 원본 데이터를 변형해서 새로운 데이터를 생성한다.

---

### **하이퍼파라미터 튜닝**

모델을 학습할 때의 여러 가지 설정값을 의미한다. 이 값을 어떻게 설정하느냐에 따라 모델의 학습 결과가 많이 바뀌게 된다.

**학습률(Learning Rate)**은 모델의 가중치를 업데이트할 때, 여기서 얼마만큼의 비율로 학습할지를 조정한다. 즉, 모델의 가중치를 업데이트하는 속도를 결정한다. **배치 크기(Batch size****)**는 한 번의 업데이트에 사용하는 데이터의 샘플 수를 결정한다. **에포크 수(Number of Epochs)**는 전체 데이터 셋을 몇 번 반복해서 학습할지를 결정한다. **모멘텀(Momentum)**는 이전 기울기를 현재 기울기에 반영하여, 학습 속도를 높이고 진동을 줄인다. **가중치 초기화(Weight Initialization)** 는 모델이 처음 만들어지면 당연히 기초값이 있을 것인데, 이 기초값을 초기화하는 방법을 설정하는 것이다. 가중치 초기화 방법에는 He 초기화, Xavier 초기화가 있다.

---

#### **모델 평가와 검증**

우리의 목적은 좋은 딥러닝 모델을 만드는 것이다. 딥러닝의 목적은 실제 문제를 잘 해결하는 것이다. 즉, 우리가 만드는 모델은 학습 데이터를 통해 학습하지만, 최종적으로는 실제 문제를 잘 풀어야 하는 것이다. 그렇기 때문에 **평가와 검증 단계**가 중요하다. 검증 방법으로는 K-Fold 교차검증이 있다.

####  **Pytorch 문법 정리**

**기본 모델**을 만드려면 torch에서 nn.Module이라는 것을 상속받은 다음, MyModel을 만들면 된다.

```
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.layer1 = nn.Linear(10, 20)

    def forward(self, x):
        x = self.layer1(x)
        return x
```

torch에서 nn.Module에서 **손실 함수**를 기본적으로 제공해준다.

```
loss_fn = nn.CrossEntropyLoss()	# 분류 문제에 주로 사용됨

loss_fn = nn.MSELoss()	# 회귀 문제에 주로 사용됨.
```

torch.optim 모듈에 다양한 최적화 알고리즘들이 포함돼있다. 우리가 수학적으로 깊게 다루지 않아도 딥러닝 모델을 만들 수 있는 이유다. 

```
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)	# 확률적 경사 하강법 최적화 알고리즘

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)	# Adam 최적화 알고리즘
```

아래 코드는 torch.utils.data.Dataset 로 사용자 정의 데이터 셋을 만들기 위한 기본 클래스다.

```
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.targets[idx]
```

아래의 코드는 torch.utils.data.Dataloader 로 미니 배치 학습을 위한 데이터 로더다.

```
from torch.utils.data import DataLoader

dataset = MyDataset(data, targets)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

torch는 데이터 변환을 위한 다양한 도구도 제공한다.

```
from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
```

GPU를 사용하려면 우리의 데이터 셋을 Tensor 텐서로 이동해야 한다. 본인의 device에 맞게 설정해야 하므로 주의해야 한다.

```
# 모델을 GPU로 이동
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# 텐서를 GPU로 이동
inputs, targets = inputs.to(device), targets.to(device)
```

**모델 기법별로 API도 제공**된다. 우리가 배운 CNN, RNN, 트랜스포머, 거기에 트랜스포머는 인코더까지 레이어로 기본 제공이 된다. 아래 코드는 Transformer의 예시 코드다.

```
# torch.nn.Transformer: 트랜스포머 모델
ransformer_model = nn.Transformer(nhead=8, num_encoder_layers=6)

# torch.nn.TransformerEncoderLayer: 트랜스포머 인코더 레이어
encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
```

또 **모델의 저장 및 로드** 기능도 있다. 모델을 저장해놔야 사용할 수 있기 때문이다. 또 모델의 모드를 **학습 모드로 설정**하거나 **평가 모드로 설정**하는 기능도 있다.

---

#### **강의 외부에서 추가로 알게 된 내용**

-   **최빈값(모드, mode)**

**데이터에서 가장 자주 등장하는 값**을 의미한다. 수치형 데이터나 범주형 데이터 모두에서 사용할 수 있다.

pandas의 mode() 함수를 사용하거나, scipy 라이브러리의 stats.mode()를 사용하여 최빈값을 구할 수 있다.

또 이보다는 비효율적이지만 collections.Counter를 사용해 빈도를 계산한 후 counter.most\_common(1) 함수를 통해 가장 빈도가 높은 값과 그 빈도를 반환하고, mode\_value\[0\]\[0\]에서 최빈값을 확인할 수 있다.